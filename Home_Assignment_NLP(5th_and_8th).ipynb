{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyQ02FsHp_-T",
        "outputId": "1437d7fc-0f37-421c-88ca-c71438a15109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per-class Precision and Recall:\n",
            "  Cat: Precision = 0.2500, Recall = 0.2500\n",
            "  Dog: Precision = 0.4444, Recall = 0.4444\n",
            "  Rabbit: Precision = 0.4000, Recall = 0.4000\n",
            "\n",
            "Macro-averaged:\n",
            "  Precision = 0.3648, Recall = 0.3648\n",
            "\n",
            "Micro-averaged:\n",
            "  Precision = 0.3889, Recall = 0.3889\n"
          ]
        }
      ],
      "source": [
        "# 5th\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "# ----- Input -----\n",
        "labels = [\"Cat\", \"Dog\", \"Rabbit\"]\n",
        "confusion_matrix = [\n",
        "    [5, 10, 5],   # Predicted Cat\n",
        "    [15, 20, 10], # Predicted Dog\n",
        "    [0, 15, 10]   # Predicted Rabbit\n",
        "]\n",
        "\n",
        "# ----- Functions -----\n",
        "def compute_metrics(labels, cm):\n",
        "    n = len(labels)\n",
        "\n",
        "    # Column sums = actual totals, Row sums = predicted totals\n",
        "    col_sums = [sum(cm[r][c] for r in range(n)) for c in range(n)]\n",
        "    row_sums = [sum(cm[r]) for r in range(n)]\n",
        "\n",
        "    results = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        tp = cm[i][i]\n",
        "        fp = row_sums[i] - tp\n",
        "        fn = col_sums[i] - tp\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "\n",
        "        results[label] = {\"precision\": precision, \"recall\": recall}\n",
        "    return results\n",
        "\n",
        "def macro_average(metrics):\n",
        "    precisions = [m[\"precision\"] for m in metrics.values()]\n",
        "    recalls = [m[\"recall\"] for m in metrics.values()]\n",
        "    return {\n",
        "        \"precision\": sum(precisions) / len(precisions),\n",
        "        \"recall\": sum(recalls) / len(recalls),\n",
        "    }\n",
        "\n",
        "def micro_average(cm):\n",
        "    tp_sum = sum(cm[i][i] for i in range(len(cm)))\n",
        "    total = sum(sum(row) for row in cm)\n",
        "    micro = tp_sum / total if total > 0 else 0.0\n",
        "    return {\"precision\": micro, \"recall\": micro}\n",
        "\n",
        "def fmt(x):\n",
        "    return f\"{x:.4f}\"\n",
        "\n",
        "# ----- Main -----\n",
        "metrics = compute_metrics(labels, confusion_matrix)\n",
        "macro = macro_average(metrics)\n",
        "micro = micro_average(confusion_matrix)\n",
        "\n",
        "print(\"Per-class Precision and Recall:\")\n",
        "for label, vals in metrics.items():\n",
        "    print(f\"  {label}: Precision = {fmt(vals['precision'])}, Recall = {fmt(vals['recall'])}\")\n",
        "\n",
        "print(\"\\nMacro-averaged:\")\n",
        "print(f\"  Precision = {fmt(macro['precision'])}, Recall = {fmt(macro['recall'])}\")\n",
        "\n",
        "print(\"\\nMicro-averaged:\")\n",
        "print(f\"  Precision = {fmt(micro['precision'])}, Recall = {fmt(micro['recall'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8th\n",
        "\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# ------------------------------\n",
        "# 1) Training corpus\n",
        "# ------------------------------\n",
        "corpus_lines = [\n",
        "    \"<s> I love NLP </s>\",\n",
        "    \"<s> I love deep learning </s>\",\n",
        "    \"<s> deep learning is fun </s>\",\n",
        "]\n",
        "\n",
        "def tokenize(line: str) -> List[str]:\n",
        "    \"\"\"Split a sentence on whitespace (keep <s> and </s>).\"\"\"\n",
        "    return line.split()\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Unigram & Bigram counts\n",
        "# ------------------------------\n",
        "def get_counts(corpus: List[str]) -> Tuple[Counter, Counter, Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      - unigrams: Counter of tokens\n",
        "      - bigrams: Counter of (w1, w2) pairs\n",
        "      - next_totals: dict mapping w1 -> total times any w2 follows w1\n",
        "    \"\"\"\n",
        "    unigrams = Counter()\n",
        "    bigrams = Counter()\n",
        "    next_totals = defaultdict(int)  # sum_x count(w_{i-1}, x) for each history word\n",
        "\n",
        "    for line in corpus:\n",
        "        toks = tokenize(line)\n",
        "        unigrams.update(toks)\n",
        "        for i in range(len(toks) - 1):\n",
        "            w1, w2 = toks[i], toks[i + 1]\n",
        "            bigrams[(w1, w2)] += 1\n",
        "            next_totals[w1] += 1\n",
        "\n",
        "    return unigrams, bigrams, next_totals\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Bigram MLE\n",
        "# ------------------------------\n",
        "def bigram_mle_prob(w2: str, w1: str, bigrams: Counter, next_totals: Dict[str, int]) -> float:\n",
        "    \"\"\"\n",
        "    MLE: P(w2 | w1) = count(w1, w2) / sum_x count(w1, x)\n",
        "    Returns 0.0 if w1 never appears (no outgoing bigrams).\n",
        "    \"\"\"\n",
        "    num = bigrams.get((w1, w2), 0)\n",
        "    den = next_totals.get(w1, 0)\n",
        "    return (num / den) if den > 0 else 0.0\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Sentence probability (product of bigrams)\n",
        "# ------------------------------\n",
        "def sentence_probability(sentence: str, bigrams: Counter, next_totals: Dict[str, int], verbose: bool = True) -> float:\n",
        "    \"\"\"\n",
        "    Computes P(w1,...,wk) = Î _i P(w_i | w_{i-1}) with bigram MLE.\n",
        "    If verbose=True, prints the step-by-step conditional probabilities.\n",
        "    \"\"\"\n",
        "    toks = tokenize(sentence)\n",
        "    p = 1.0\n",
        "    steps = []\n",
        "    for i in range(len(toks) - 1):\n",
        "        w1, w2 = toks[i], toks[i + 1]\n",
        "        prob = bigram_mle_prob(w2, w1, bigrams, next_totals)\n",
        "        steps.append((w1, w2, prob))\n",
        "        p *= prob\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nSentence: {sentence}\")\n",
        "        for w1, w2, prob in steps:\n",
        "            print(f\"  P({w2}|{w1}) = {prob:.6f}\")\n",
        "        print(f\"  => Sentence probability = {p:.6f}\")\n",
        "    return p\n",
        "\n",
        "# ------------------------------\n",
        "# 5) Driver\n",
        "# ------------------------------\n",
        "def main():\n",
        "    print(\"=== Bigram Language Model (MLE) on the 'I love NLP' corpus ===\")\n",
        "    print(\"Theory note: P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / sum_x count(w_{i-1}, x)\")\n",
        "    print(\"We multiply these bigram probabilities across a sentence to get its probability.\\n\")\n",
        "\n",
        "    uni, bi, next_totals = get_counts(corpus_lines)\n",
        "\n",
        "    # Show compact counts useful for the report\n",
        "    print(\"Unigram counts (subset):\")\n",
        "    for tok in [\"<s>\", \"I\", \"love\", \"NLP\", \"deep\", \"learning\", \"is\", \"fun\", \"</s>\"]:\n",
        "        print(f\"  {tok}: {uni.get(tok, 0)}\")\n",
        "\n",
        "    print(\"\\nSome Bigram counts:\")\n",
        "    show_pairs = [\n",
        "        (\"<s>\", \"I\"), (\"<s>\", \"deep\"),\n",
        "        (\"I\", \"love\"),\n",
        "        (\"love\", \"NLP\"), (\"love\", \"deep\"),\n",
        "        (\"deep\", \"learning\"),\n",
        "        (\"learning\", \"</s>\"), (\"learning\", \"is\"),\n",
        "        (\"is\", \"fun\"),\n",
        "        (\"fun\", \"</s>\"),\n",
        "        (\"NLP\", \"</s>\"),\n",
        "    ]\n",
        "    for w1, w2 in show_pairs:\n",
        "        print(f\"  count({w1!r},{w2!r}) = {bi.get((w1, w2), 0)}; total after {w1!r} = {next_totals.get(w1, 0)}\")\n",
        "\n",
        "    # Test sentences\n",
        "    s1 = \"<s> I love NLP </s>\"\n",
        "    s2 = \"<s> I love deep learning </s>\"\n",
        "\n",
        "    p1 = sentence_probability(s1, bi, next_totals, verbose=True)\n",
        "    p2 = sentence_probability(s2, bi, next_totals, verbose=True)\n",
        "\n",
        "    # Decision + explanation\n",
        "    print(\"\\n=== Decision ===\")\n",
        "    if p1 > p2:\n",
        "        print(\"Model prefers S1 =\", s1)\n",
        "        print(f\"Reason: P(S1) = {p1:.6f} is greater than P(S2) = {p2:.6f}.\")\n",
        "        print(\"Why: both paths share early factors, but S2 ends with P(</s>|learning)=1/2 while S1 ends with P(</s>|NLP)=1.\")\n",
        "    elif p2 > p1:\n",
        "        print(\"Model prefers S2 =\", s2)\n",
        "        print(f\"Reason: P(S2) = {p2:.6f} is greater than P(S1) = {p1:.6f}.\")\n",
        "    else:\n",
        "        print(\"Model is indifferent: P(S1) = P(S2) =\", f\"{p1:.6f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6WRHYWkqPqr",
        "outputId": "c4df1dcd-4201-43e5-c34f-e038e8893761"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Bigram Language Model (MLE) on the 'I love NLP' corpus ===\n",
            "Theory note: P(w_i | w_{i-1}) = count(w_{i-1}, w_i) / sum_x count(w_{i-1}, x)\n",
            "We multiply these bigram probabilities across a sentence to get its probability.\n",
            "\n",
            "Unigram counts (subset):\n",
            "  <s>: 3\n",
            "  I: 2\n",
            "  love: 2\n",
            "  NLP: 1\n",
            "  deep: 2\n",
            "  learning: 2\n",
            "  is: 1\n",
            "  fun: 1\n",
            "  </s>: 3\n",
            "\n",
            "Some Bigram counts:\n",
            "  count('<s>','I') = 2; total after '<s>' = 3\n",
            "  count('<s>','deep') = 1; total after '<s>' = 3\n",
            "  count('I','love') = 2; total after 'I' = 2\n",
            "  count('love','NLP') = 1; total after 'love' = 2\n",
            "  count('love','deep') = 1; total after 'love' = 2\n",
            "  count('deep','learning') = 2; total after 'deep' = 2\n",
            "  count('learning','</s>') = 1; total after 'learning' = 2\n",
            "  count('learning','is') = 1; total after 'learning' = 2\n",
            "  count('is','fun') = 1; total after 'is' = 1\n",
            "  count('fun','</s>') = 1; total after 'fun' = 1\n",
            "  count('NLP','</s>') = 1; total after 'NLP' = 1\n",
            "\n",
            "Sentence: <s> I love NLP </s>\n",
            "  P(I|<s>) = 0.666667\n",
            "  P(love|I) = 1.000000\n",
            "  P(NLP|love) = 0.500000\n",
            "  P(</s>|NLP) = 1.000000\n",
            "  => Sentence probability = 0.333333\n",
            "\n",
            "Sentence: <s> I love deep learning </s>\n",
            "  P(I|<s>) = 0.666667\n",
            "  P(love|I) = 1.000000\n",
            "  P(deep|love) = 0.500000\n",
            "  P(learning|deep) = 1.000000\n",
            "  P(</s>|learning) = 0.500000\n",
            "  => Sentence probability = 0.166667\n",
            "\n",
            "=== Decision ===\n",
            "Model prefers S1 = <s> I love NLP </s>\n",
            "Reason: P(S1) = 0.333333 is greater than P(S2) = 0.166667.\n",
            "Why: both paths share early factors, but S2 ends with P(</s>|learning)=1/2 while S1 ends with P(</s>|NLP)=1.\n"
          ]
        }
      ]
    }
  ]
}